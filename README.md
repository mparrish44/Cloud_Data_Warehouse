# ğŸš€ AWS Redshift Data Warehouse & ETL Pipeline

## **ğŸ’Œ Project Overview**
This project builds an **ETL pipeline** for **Sparkify**, a music streaming startup, using **Amazon Redshift**.  
The pipeline extracts **JSON log & song data** from **AWS S3**, stages it in **Redshift**, and transforms it into a **star schema** for analytical queries.  

---

## **ğŸ“‰ Database Schema**
The **star schema** consists of:
- **Fact Table**: `songplays` - Stores records of song play activity.
- **Dimension Tables**: `users`, `songs`, `artists`, `time`.

### **Schema Diagram**
```
        songplays (fact)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ songplay_id â”‚
        â”‚ start_time  â”‚
        â”‚ user_id     â”‚
        â”‚ level       â”‚
        â”‚ song_id     â”‚
        â”‚ artist_id   â”‚
        â”‚ session_id  â”‚
        â”‚ location    â”‚
        â”‚ user_agent  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–²  
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   users   â”‚â”‚  songs   â”‚â”‚ artists  â”‚â”‚   time   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **ğŸ“š Repository Structure**
| File | Description |
|------|------------|
| `create_tables.py` | Drops and creates Redshift tables before running ETL. |
| `etl.py` | Extracts, Loads, and Transforms (ETL) data from S3 to Redshift. |
| `sql_queries.py` | Contains SQL queries for creating, copying, and inserting data. |
| `dwh.cfg` | Configuration file storing AWS Redshift, IAM, and S3 settings. |
| `README.md` | This documentation file. |

---

## **ğŸ’¼ How to Run This Project**

### **1ï¸âƒ£ Setup AWS Resources**
1. Create an **IAM Role** with `AmazonS3ReadOnlyAccess` and attach it to Redshift.
2. Create a **Redshift Cluster** and note:
   - **Cluster Endpoint**
   - **Database Name**
   - **Username & Password**
3. Update `dwh.cfg` with these values.

### **2ï¸âƒ£ Run Python Scripts**
#### **ğŸ”¹ Install Required Libraries**
Run:
```bash
pip install psycopg2 configparser
```

#### **ğŸ”¹ Run `create_tables.py`**
This **drops and recreates tables** in Redshift:
```bash
python create_tables.py
```

#### **ğŸ”¹ Run `etl.py`**
This **loads data from S3 â†’ Redshift staging tables â†’ Star schema**:
```bash
python etl.py
```

#### **ğŸ”¹ Verify in Redshift**
Run:
```sql
1. SELECT COUNT(*) FROM songplays;
2. SELECT COUNT(*) FROM users;
3. SELECT COUNT(*) FROM songs;
4. SELECT COUNT(*) FROM artists;
5. SELECT COUNT(*) FROM time;
6. SELECT * FROM time LIMIT 5;
7. SELECT * FROM staging_events LIMIT 5;
8. SELECT * FROM staging_songs LIMIT 5;
9. SELECT s.title AS song, count(song) AS frequency 
10. FROM songplays sp 
    JOIN songs s ON (s.song_id = sp.song_id) 
    GROUP BY song
    ORDER BY count(*) desc
    LIMIT 5;
11. SELECT extract(hour from sp.start_time) as hour, count(hour) AS frequency
    FROM songplays sp
    GROUP BY hour
    ORDER BY count(*) desc
    LIMIT 5;


```
âœ… If row counts are greater than `0`, ETL **ran successfully**!

---

## **ğŸ› ï¸ Cleanup (Avoid AWS Charges)**
Once done, **delete your Redshift Cluster**:
1. **Go to AWS Redshift Console**.
2. Select your cluster â†’ **Click "Delete"**.

---

## **ğŸš€ Next Steps**
âœ… Optimize queries for performance.  
âœ… Use **Apache Airflow** to schedule & automate the pipeline.  
âœ… Deploy **multi-node Redshift cluster** for scalability.  


